{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b30b0c-fb6b-421d-b1e2-c5b93f64a184",
   "metadata": {
    "id": "e1b30b0c-fb6b-421d-b1e2-c5b93f64a184"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n================================================================================\\nPredictive Maintenance of Industrial Machinery - High-Accuracy Classification\\n================================================================================\\n\\nProject Objective:\\n------------------\\nThis notebook details the end-to-end process of developing a high-accuracy\\nmachine learning model (>=98%) to predict specific failure types in industrial\\nmachinery. The model is trained on sensor data from the Kaggle \"Predictive\\nMaintenance\" dataset.\\n\\nMethodology:\\n------------\\n1.  **Data Loading & Preparation**: Securely load the dataset from IBM Cloud\\n    Object Storage and perform initial cleaning and preparation.\\n2.  **Preprocessing**: Create a robust preprocessing pipeline to handle both\\n    numerical (scaling) and categorical (encoding) data types.\\n3.  **Handling Class Imbalance**: Utilize the SMOTE (Synthetic Minority\\n    Over-sampling Technique) to address the significant class imbalance,\\n    ensuring the model learns from rare failure events.\\n4.  **Model Training & Hyperparameter Tuning**: Train an XGBoost classifier, a\\n    powerful gradient-boosting algorithm. Hyperparameters are tuned using a\\n    manual grid search with cross-validation, a robust method chosen to\\n    bypass environment-specific library conflicts.\\n5.  **Evaluation**: Rigorously evaluate the final model on an unseen test set\\n    using a suite of metrics, including accuracy, precision, recall, F1-score,\\n    and a confusion matrix.\\n6.  **Model Serialization**: Save the final, deployment-ready model pipeline\\n    and the label encoder for future use in a production environment.\\n\\nAuthor:\\n-------\\nSai Abhinav Patel Sadineni\\nAI model developed for project requirements.\\n\\nLast Updated:\\n-------------\\nJuly 27, 2025\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Predictive Maintenance of Industrial Machinery - High-Accuracy Classification\n",
    "================================================================================\n",
    "\n",
    "Project Objective:\n",
    "------------------\n",
    "This notebook details the end-to-end process of developing a high-accuracy\n",
    "machine learning model (>=98%) to predict specific failure types in industrial\n",
    "machinery. The model is trained on sensor data from the Kaggle \"Predictive\n",
    "Maintenance\" dataset.\n",
    "\n",
    "Methodology:\n",
    "------------\n",
    "1.  **Data Loading & Preparation**: Securely load the dataset from IBM Cloud\n",
    "    Object Storage and perform initial cleaning and preparation.\n",
    "2.  **Preprocessing**: Create a robust preprocessing pipeline to handle both\n",
    "    numerical (scaling) and categorical (encoding) data types.\n",
    "3.  **Handling Class Imbalance**: Utilize the SMOTE (Synthetic Minority\n",
    "    Over-sampling Technique) to address the significant class imbalance,\n",
    "    ensuring the model learns from rare failure events.\n",
    "4.  **Model Training & Hyperparameter Tuning**: Train an XGBoost classifier, a\n",
    "    powerful gradient-boosting algorithm. Hyperparameters are tuned using a\n",
    "    manual grid search with cross-validation, a robust method chosen to\n",
    "    bypass environment-specific library conflicts.\n",
    "5.  **Evaluation**: Rigorously evaluate the final model on an unseen test set\n",
    "    using a suite of metrics, including accuracy, precision, recall, F1-score,\n",
    "    and a confusion matrix.\n",
    "6.  **Model Serialization**: Save the final, deployment-ready model pipeline\n",
    "    and the label encoder for future use in a production environment.\n",
    "\n",
    "Author:\n",
    "-------\n",
    "Sai Abhinav Patel Sadineni\n",
    "AI model developed for project requirements.\n",
    "\n",
    "Last Updated:\n",
    "-------------\n",
    "July 27, 2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b9b09c-56e5-4ad3-9a29-b9b5ec801b4f",
   "metadata": {
    "id": "e9b9b09c-56e5-4ad3-9a29-b9b5ec801b4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-25.1.1\n",
      "Requirement already satisfied: xgboost in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (2.0.3)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost)\n",
      "  Downloading nvidia_nccl_cu12-2.27.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from xgboost) (1.11.4)\n",
      "Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m137.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 MB\u001b[0m \u001b[31m137.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
      "\u001b[2K  Attempting uninstall: xgboost━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: xgboost 2.0.30m \u001b[32m0/2\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling xgboost-2.0.3:━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled xgboost-2.0.390m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [xgboost]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [xgboost]m1/2\u001b[0m [xgboost]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autoai-libs 2.0.21 requires xgboost==2.0.*, but you have xgboost 3.0.2 which is incompatible.\n",
      "autoai-ts-libs 4.0.18 requires xgboost==2.0.*, but you have xgboost 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.27.6 xgboost-3.0.2\n",
      "Collecting imbalanced-learn==0.11.0\n",
      "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from imbalanced-learn==0.11.0) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from imbalanced-learn==0.11.0) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from imbalanced-learn==0.11.0) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from imbalanced-learn==0.11.0) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from imbalanced-learn==0.11.0) (2.2.0)\n",
      "Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.11.0\n",
      "Requirement already satisfied: ibm-watson-machine-learning in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (1.0.368)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.32.4)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (1.26.19)\n",
      "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.1.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2025.6.15)\n",
      "Requirement already satisfied: lomond in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (0.3.3)\n",
      "Requirement already satisfied: tabulate in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (0.8.10)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (23.2)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.15.0,>=2.12.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.14.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (7.0.1)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.14.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watson-machine-learning) (2.14.2)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watson-machine-learning) (2.14.2)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watson-machine-learning) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.14.2->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watson-machine-learning) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.9.0->ibm-cos-sdk-core==2.14.2->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watson-machine-learning) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->ibm-watson-machine-learning) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->ibm-watson-machine-learning) (3.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from importlib-metadata->ibm-watson-machine-learning) (3.20.2)\n",
      "Requirement already satisfied: ibm-cos-sdk in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (2.14.2)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.14.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk) (2.14.2)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk) (2.14.2)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.14.2->ibm-cos-sdk) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.4 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.14.2->ibm-cos-sdk) (2.32.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.18 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.14.2->ibm-cos-sdk) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.9.0->ibm-cos-sdk-core==2.14.2->ibm-cos-sdk) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.4->ibm-cos-sdk-core==2.14.2->ibm-cos-sdk) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.4->ibm-cos-sdk-core==2.14.2->ibm-cos-sdk) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.4->ibm-cos-sdk-core==2.14.2->ibm-cos-sdk) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 1. ENVIRONMENT SETUP\n",
    "# ===================================================================\n",
    "# This cell installs all the necessary libraries. It's recommended to\n",
    "# run this cell first, then restart the kernel (from the menu:\n",
    "# Kernel -> Restart) before running the rest of the notebook.\n",
    "# ===================================================================\n",
    "!pip install -U pip\n",
    "!pip install -U xgboost\n",
    "!pip install -U imbalanced-learn==0.11.0\n",
    "!pip install -U ibm-watson-machine-learning\n",
    "!pip install -U ibm-cos-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdacb02-cb9e-4b85-998b-df48c9ec03e9",
   "metadata": {
    "id": "abdacb02-cb9e-4b85-998b-df48c9ec03e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn==1.3) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn==1.3) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn==1.3) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn==1.3) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d5fb06-17ec-45f6-8fb4-6871fb670af1",
   "metadata": {
    "id": "80d5fb06-17ec-45f6-8fb4-6871fb670af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77d7370-d091-46bf-81f9-5879bc4b2d95",
   "metadata": {
    "id": "b77d7370-d091-46bf-81f9-5879bc4b2d95"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 2. IMPORT LIBRARIES\n",
    "# ===================================================================\n",
    "# This cell imports all the Python libraries required for the pipeline.\n",
    "# ===================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import types\n",
    "import warnings\n",
    "import os, types\n",
    "\n",
    "# --- IBM Cloud Object Storage ---\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "# --- Preprocessing ---\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- Imbalanced Data Handling ---\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Model ---\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- IBM Watsonx Libraries ---\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "# --- General Settings ---\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0445a7b-a543-4027-b10d-eee2090045ad",
   "metadata": {
    "id": "f0445a7b-a543-4027-b10d-eee2090045ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Step 1 & 2: Data Loading and Initial Preprocessing ###\n",
      "Dataset loaded successfully from cloud.\n",
      "   UDI Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
      "0    1     M14860    M                298.1                    308.6   \n",
      "1    2     L47181    L                298.2                    308.7   \n",
      "2    3     L47182    L                298.1                    308.5   \n",
      "3    4     L47183    L                298.2                    308.6   \n",
      "4    5     L47184    L                298.2                    308.7   \n",
      "5    6     M14865    M                298.1                    308.6   \n",
      "6    7     L47186    L                298.1                    308.6   \n",
      "7    8     L47187    L                298.1                    308.6   \n",
      "8    9     M14868    M                298.3                    308.7   \n",
      "9   10     M14869    M                298.5                    309.0   \n",
      "\n",
      "   Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Target Failure Type  \n",
      "0                    1551         42.8                0       0   No Failure  \n",
      "1                    1408         46.3                3       0   No Failure  \n",
      "2                    1498         49.4                5       0   No Failure  \n",
      "3                    1433         39.5                7       0   No Failure  \n",
      "4                    1408         40.0                9       0   No Failure  \n",
      "5                    1425         41.9               11       0   No Failure  \n",
      "6                    1558         42.4               14       0   No Failure  \n",
      "7                    1527         40.2               16       0   No Failure  \n",
      "8                    1667         28.6               18       0   No Failure  \n",
      "9                    1741         28.0               21       0   No Failure  \n",
      "\n",
      "Updated data types:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   UDI                      10000 non-null  int64  \n",
      " 1   Product ID               10000 non-null  object \n",
      " 2   Type                     10000 non-null  object \n",
      " 3   Air temperature [K]      10000 non-null  float64\n",
      " 4   Process temperature [K]  10000 non-null  float64\n",
      " 5   Rotational speed [rpm]   10000 non-null  int64  \n",
      " 6   Torque [Nm]              10000 non-null  float64\n",
      " 7   Tool wear [min]          10000 non-null  int64  \n",
      " 8   Target                   10000 non-null  int64  \n",
      " 9   Failure Type             10000 non-null  object \n",
      "dtypes: float64(3), int64(4), object(3)\n",
      "memory usage: 781.4+ KB\n",
      "None\n",
      "\n",
      "LabelEncoder has been fitted on all data.\n",
      "\n",
      "Data split into training (8000 rows) and testing (2000 rows) sets.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 3. DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ===================================================================\n",
    "print(\"### Step 1 & 2: Data Loading and Initial Preprocessing ###\")\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='INSERT_YOUR_API_KEY_HERE',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/identity/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.direct.us-south.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "bucket = 'predictivemaintenanceproject-donotdelete-pr-nfnojko53chusc'\n",
    "object_key = 'predictive_maintenance.csv'\n",
    "\n",
    "body = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "df = pd.read_csv(body)\n",
    "print(\"Dataset loaded successfully from cloud.\")\n",
    "print(df.head(10))\n",
    "\n",
    "# List of columns that should be numeric\n",
    "numeric_cols = [\n",
    "    'Air temperature [K]',\n",
    "    'Process temperature [K]',\n",
    "    'Rotational speed [rpm]',\n",
    "    'Torque [Nm]',\n",
    "    'Tool wear [min]'\n",
    "]\n",
    "\n",
    "# Convert each column to a numeric type.\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Check for and remove any rows that now have missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(f\"Original number of rows: {len(df)}\")\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"Removed rows with non-numeric data. New number of rows: {len(df)}\")\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nUpdated data types:\")\n",
    "print(df.info())\n",
    "\n",
    "# Initial cleaning and data splitting\n",
    "df_cleaned = df.drop(['UDI', 'Product ID'], axis=1)\n",
    "X = df_cleaned.drop(['Target', 'Failure Type'], axis=1)\n",
    "y = df_cleaned['Failure Type']\n",
    "\n",
    "# Encode the target variable for the entire dataset\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(\"\\nLabelEncoder has been fitted on all data.\")\n",
    "\n",
    "# Split into training and testing sets, ensuring stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "print(f\"\\nData split into training ({X_train.shape[0]} rows) and testing ({X_test.shape[0]} rows) sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e65ecf2-c40b-4069-8f72-8e28bf02ee90",
   "metadata": {
    "id": "0e65ecf2-c40b-4069-8f72-8e28bf02ee90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Step 3: Setting up the Preprocessing Pipeline ###\n",
      "Fitting preprocessor and transforming training data...\n",
      "Training and testing data have been processed.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 4. PREPROCESSING PIPELINE SETUP\n",
    "# ===================================================================\n",
    "print(\"\\n### Step 3: Setting up the Preprocessing Pipeline ###\")\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['Type']\n",
    "numerical_features = X.columns.drop(categorical_features).tolist()\n",
    "\n",
    "# Create a ColumnTransformer to apply different transformations to different columns\n",
    "# - StandardScaler for numerical features\n",
    "# - OneHotEncoder for categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the training data and transform both sets\n",
    "print(\"Fitting preprocessor and transforming training data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(\"Training and testing data have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a45f35e-40bc-42f4-9567-aeaca92258d3",
   "metadata": {
    "id": "0a45f35e-40bc-42f4-9567-aeaca92258d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Step 4: Model Development (Handling Imbalance & Manual Grid Search) ###\n",
      "Applying SMOTE to the training data...\n",
      "Data resampled. New training shape: (46332, 8)\n",
      "\n",
      "Starting manual grid search...\n",
      "Testing params: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "  -> CV Score: 0.9935\n",
      "Testing params: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.2}\n",
      "  -> CV Score: 0.9955\n",
      "Testing params: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1}\n",
      "  -> CV Score: 0.9954\n",
      "Testing params: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.2}\n",
      "  -> CV Score: 0.9958\n",
      "Testing params: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "  -> CV Score: 0.9949\n",
      "Testing params: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2}\n",
      "  -> CV Score: 0.9959\n",
      "Testing params: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1}\n",
      "  -> CV Score: 0.9959\n",
      "Testing params: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
      "  -> CV Score: 0.9960\n",
      "\n",
      "Manual grid search complete.\n",
      "Best cross-validation accuracy: 0.9960\n",
      "Best parameters found: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
      "\n",
      "Training final model with best parameters...\n",
      "Final model trained.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 5. HANDLE CLASS IMBALANCE & TUNE MODEL (MANUAL GRID SEARCH)\n",
    "# ===================================================================\n",
    "print(\"\\n### Step 4: Model Development (Handling Imbalance & Manual Grid Search) ###\")\n",
    "\n",
    "# --- Apply SMOTE directly to the processed training data ---\n",
    "print(\"Applying SMOTE to the training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "print(f\"Data resampled. New training shape: {X_train_resampled.shape}\")\n",
    "\n",
    "# --- Manually tune the XGBoost model ---\n",
    "# This approach avoids potential issues with GridSearchCV in some cloud environments.\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [5, 7],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\nStarting manual grid search...\")\n",
    "\n",
    "# Loop through each combination of parameters\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "            current_params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate}\n",
    "            fold_scores = []\n",
    "            print(f\"Testing params: {current_params}\")\n",
    "\n",
    "            # Perform cross-validation for the current parameter set\n",
    "            for train_idx, val_idx in cv.split(X_train_resampled, y_train_resampled):\n",
    "                X_train_fold, X_val_fold = X_train_resampled[train_idx], X_train_resampled[val_idx]\n",
    "                y_train_fold, y_val_fold = y_train_resampled[train_idx], y_train_resampled[val_idx]\n",
    "                \n",
    "                model = xgb.XGBClassifier(objective='multi:softmax', random_state=42, eval_metric='mlogloss', **current_params)\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                preds = model.predict(X_val_fold)\n",
    "                score = accuracy_score(y_val_fold, preds)\n",
    "                fold_scores.append(score)\n",
    "            \n",
    "            avg_score = np.mean(fold_scores)\n",
    "            print(f\"  -> CV Score: {avg_score:.4f}\")\n",
    "\n",
    "            # Update best score and parameters if current model is better\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = current_params\n",
    "\n",
    "print(\"\\nManual grid search complete.\")\n",
    "print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Train the final best model on all the resampled data\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_xgb_model = xgb.XGBClassifier(objective='multi:softmax', random_state=42, eval_metric='mlogloss', **best_params)\n",
    "best_xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Final model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37be154f-cae2-4db5-b4da-e98fa886b5af",
   "metadata": {
    "id": "37be154f-cae2-4db5-b4da-e98fa886b5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Step 5: Model Evaluation ###\n",
      "\n",
      "Test Accuracy: 0.9770\n",
      "⚠️ Target accuracy of >= 98% was not met.\n",
      "\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "Heat Dissipation Failure       0.88      0.95      0.91        22\n",
      "              No Failure       0.99      0.98      0.99      1930\n",
      "      Overstrain Failure       0.80      1.00      0.89        16\n",
      "           Power Failure       0.89      0.89      0.89        19\n",
      "         Random Failures       0.00      0.00      0.00         4\n",
      "       Tool Wear Failure       0.00      0.00      0.00         9\n",
      "\n",
      "                accuracy                           0.98      2000\n",
      "               macro avg       0.59      0.64      0.61      2000\n",
      "            weighted avg       0.98      0.98      0.98      2000\n",
      "\n",
      "\n",
      "NOTE: A score of 0.00 for rare classes like 'Random Failures' indicates that the model did not learn to predict them due to extreme data imbalance.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 6. EVALUATION\n",
    "# ===================================================================\n",
    "print(\"\\n### Step 5: Model Evaluation ###\")\n",
    "\n",
    "# Make predictions on the original (but processed) test data\n",
    "y_pred = best_xgb_model.predict(X_test_processed)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if accuracy >= 0.98:\n",
    "    print(\"✅ Target accuracy of >= 98% has been achieved!\")\n",
    "else:\n",
    "    print(\"⚠️ Target accuracy of >= 98% was not met.\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(\"\\nNOTE: A score of 0.00 for rare classes like 'Random Failures' indicates that the model did not learn to predict them due to extreme data imbalance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ad4042-635f-4497-a587-a57ef76fc30b",
   "metadata": {
    "id": "03ad4042-635f-4497-a587-a57ef76fc30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Step 6: Saving Final Deployment Pipeline and Encoder ###\n",
      "Final deployment pipeline saved to 'final_model.pkl'\n",
      "Label encoder saved to 'label_encoder.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 7. SAVE FINAL DEPLOYMENT PIPELINE\n",
    "# ===================================================================\n",
    "print(\"\\n### Step 6: Saving Final Deployment Pipeline and Encoder ###\")\n",
    "\n",
    "# For deployment, we create a final pipeline that chains the preprocessor\n",
    "# and the best XGBoost model. This single object contains the entire workflow.\n",
    "final_deployment_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', best_xgb_model)\n",
    "])\n",
    "\n",
    "# Save the complete deployment pipeline to a file\n",
    "joblib.dump(final_deployment_pipeline, 'final_model.pkl')\n",
    "print(\"Final deployment pipeline saved to 'final_model.pkl'\")\n",
    "\n",
    "# Save the label encoder, which is needed to decode predictions\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "print(\"Label encoder saved to 'label_encoder.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051efc88-67f6-486c-9f1a-32490e13b113",
   "metadata": {
    "id": "051efc88-67f6-486c-9f1a-32490e13b113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Step 7: Saving Model to Watson Machine Learning Repository ###\n",
      "\n",
      "SUCCESS: Watson Machine Learning client is configured.\n",
      "\n",
      "Storing the complete pipeline ('Predictive Maintenance Pipeline Model') in the repository...\n",
      "\n",
      "SUCCESS: Model saved to repository with ID: bf22b4dc-a3a0-44be-b116-8ad1763f977c\n",
      "You can now go to your deployment space to create a deployment from this model asset.\n",
      "\n",
      "--- Project Complete ---\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 8: SAVE MODEL TO WATSON MACHINE LEARNING REPOSITORY \n",
    "# ===================================================================\n",
    "print(\"\\n### Step 7: Saving Model to Watson Machine Learning Repository ###\")\n",
    "\n",
    "# --- ACTION REQUIRED: PASTE YOUR CREDENTIALS BELOW ---\n",
    "api_key = 'INSERT_YOUR_API_KEY_HERE'\n",
    "location = 'us-south'\n",
    "space_id = 'INSERT_YOUR_SPACE_ID_HERE'\n",
    "\n",
    "wml_credentials = {\n",
    "    \"apikey\": api_key,\n",
    "    \"url\": f'https://{location}.ml.cloud.ibm.com'\n",
    "}\n",
    "\n",
    "# Create the API client instance and set the default space.\n",
    "try:\n",
    "    client = APIClient(wml_credentials)\n",
    "    client.set.default_space(space_id)\n",
    "    print('\\nSUCCESS: Watson Machine Learning client is configured.')\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not connect to WML client. Please check credentials. Error: {e}\")\n",
    "    # Stop execution if connection fails\n",
    "    raise SystemExit(\"WML connection failed.\")\n",
    "\n",
    "\n",
    "# --- Storing the Model ---\n",
    "model_name = \"Predictive Maintenance Pipeline Model\"\n",
    "software_spec_name = \"runtime-24.1-py3.11\"\n",
    "software_spec_uid = client.software_specifications.get_id_by_name(software_spec_name)\n",
    "\n",
    "# Define the model's metadata.\n",
    "# The 'type' is required and must match the scikit-learn version in the runtime.\n",
    "metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: model_name,\n",
    "    client.repository.ModelMetaNames.TYPE: 'scikit-learn_1.3', # <-- ADD THIS LINE BACK WITH THE CORRECT VERSION\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid\n",
    "}\n",
    "\n",
    "print(f\"\\nStoring the complete pipeline ('{model_name}') in the repository...\")\n",
    "\n",
    "# Store the final_deployment_pipeline object in the repository.\n",
    "published_model = client.repository.store_model(\n",
    "    model=final_deployment_pipeline, # This is the key object to store\n",
    "    meta_props=metadata,\n",
    "    training_data=X_train, # Use original training data for schema reference\n",
    "    training_target=y_train\n",
    ")\n",
    "\n",
    "# Get the unique ID of the saved model.\n",
    "published_model_uid = client.repository.get_model_id(published_model)\n",
    "\n",
    "print(f\"\\nSUCCESS: Model saved to repository with ID: {published_model_uid}\")\n",
    "print(\"You can now go to your deployment space to create a deployment from this model asset.\")\n",
    "print(\"\\n--- Project Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22768d57-ce2f-4166-a51e-3377c796322b",
   "metadata": {
    "id": "22768d57-ce2f-4166-a51e-3377c796322b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
